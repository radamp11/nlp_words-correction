{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_words_correction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radamp11/nlp_words_correction/blob/develop/nlp_words_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "rUy8d7tiipdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IPx29v6QqsWI",
        "outputId": "c2375b0f-b72e-41af-c519-bbd6204c0acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P7yPOHNga1P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello world')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc0SE4yrgkuc",
        "outputId": "9bc3d383-02a2-455f-a75e-df98fbd8ce5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # if we need e.g. saving to spreadsheets on google drive\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# # if we need to connect our google drive for e.g. load text corpora\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LaJ5r5F6pWBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOOLS INSTALLATION"
      ],
      "metadata": {
        "id": "ZxjeHk1PizoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing essentiall tools via !pip install -U\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "k6Sc80whi17g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d654d92-7d69-40e5-99a5-72774e83c538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n",
            "Collecting en-core-web-lg==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.3.0/en_core_web_lg-3.3.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 400.7 MB 6.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (8.0.16)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCABULARY AND DATASET CLASS\n"
      ],
      "metadata": {
        "id": "Ao8FZBEQiwjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reads words from corpora and embeds them into tokens using sentencepiece and collects them in vocabulary dict\n",
        "class Vocabulary:\n",
        "    pass"
      ],
      "metadata": {
        "id": "OFT369OUkQCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uses Vocabulary class; returns item via __getitem__\n",
        "class WordsDataset(Dataset):\n",
        "    def __init__(self, root_dir, text_corpora_file, freq_threshold=5):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pass"
      ],
      "metadata": {
        "id": "6kYuFnvXi72z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional - may be not necessary\n",
        "# collate is needed for batch processing to fill the len difference between sentences with <PAD> token to make them the same length \n",
        "class Collate:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Sbxf7_TKmOBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL"
      ],
      "metadata": {
        "id": "_gUfbxqDi4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, input): \n",
        "        # TODO\n",
        "        input = self.dropout(input) # embed?\n",
        "        hiddens,  = self.lstm(input)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Z2oi2JANjVYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARAMETERS"
      ],
      "metadata": {
        "id": "P1WEfuIxndRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 1024 #?\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "dropout = 0.4\n",
        "vocab_size = 0 # len of dataset\n",
        "\n",
        "checkpoint_filename = (f\"date:{str(datetime.datetime.now())}\"\n",
        "                       f\"_embed:{str(embed_size)}\"\n",
        "                       f\"_hidden:{str(hidden_size)}\"\n",
        "                       f\"_lay:{str(num_layers)}\"\n",
        "                       f\"_lr:{str(learning_rate)}\"\n",
        "                       f\"_epochs:{str(num_epochs)}\"\n",
        "                       f\"_batch:{str(batch_size)}\"\n",
        "                       f\"_drop:{str(dropout)}\"\n",
        "                       f\".pth.tar\")"
      ],
      "metadata": {
        "id": "x5jRXNjjnczX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADERS"
      ],
      "metadata": {
        "id": "M_lzj8z8m12p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(root_dir, text_corpora_file, freq_threshold):\n",
        "    loader = DataLoader()\n",
        "    dataset = WordsDataset(root_dir, text_corpora_file, freq_threshold)\n",
        "    return loader, dataset"
      ],
      "metadata": {
        "id": "5rMpv1EHnAph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, dataset = get_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "unjIbYb0m3UV",
        "outputId": "d5d5ac97-23f7-4d11-a586-ebb2ebde8c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7249072817ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_loader() missing 3 required positional arguments: 'root_dir', 'text_corpora_file', and 'freq_threshold'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "x_XAkTt4oBZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "start = datetime.datetime.now()\n",
        "print(\"Training initialized at: \", start)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for idx, (stuff_from_getitem_method1, stuff2) in enumerate(train_loader):\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            stuff_from_getitem_method1 = stuff_from_getitem_method1.to(device)\n",
        "            stuff2 = stuff2.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, _ = model(stuff_from_getitem_method1, stuff2)\n",
        "\n",
        "            # loss = criterion(outputs.reshape(-1, outputs.shape[2]), reference_text_with_no_errors[1:].reshape(-1))\n",
        "            # loss.backward(loss)\n",
        "            optimizer.step()   \n",
        "\n",
        "    # saving to checkpoint step\n",
        "    if epoch % 10 == 0 and epoch != 0:\n",
        "        torch.save([model, optimizer], 'path_to_your_google_drive' + f'epoch{str(epoch)}_' + checkpoint_filename)\n",
        "\n",
        "    # validate(model, device, val_loader, dataset, f'add_att_1024_1layer_fixed_loss/add_att_1024_1layer_epoch{str(epoch)}_loss{str(round(loss.item(),6))}_' + result_worksheet_name, vis_att=True)\n",
        "    model.train()\n",
        "        \n",
        "    # print(\"Epoch no. \", epoch, \", timestamp: \", datetime.datetime.now())\n",
        "    # print(\"Loss: {:.6f}...\".format(loss.item()))\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(f\"The training took {end - start} seconds\")"
      ],
      "metadata": {
        "id": "rnKmSiEmoDDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING CONFUSION SET"
      ],
      "metadata": {
        "id": "hjGHWaC6qTNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy as spacy\n",
        "import regex as re\n",
        "import json\n",
        "\n",
        "\n",
        "def levenshtein_distance(token1, token2):\n",
        "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
        "\n",
        "    for t1 in range(len(token1) + 1):\n",
        "        distances[t1][0] = t1\n",
        "\n",
        "    for t2 in range(len(token2) + 1):\n",
        "        distances[0][t2] = t2\n",
        "\n",
        "    a = 0\n",
        "    b = 0\n",
        "    c = 0\n",
        "\n",
        "    for t1 in range(1, len(token1) + 1):\n",
        "        for t2 in range(1, len(token2) + 1):\n",
        "            if token1[t1 - 1] == token2[t2 - 1]:\n",
        "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
        "            else:\n",
        "                a = distances[t1][t2 - 1]\n",
        "                b = distances[t1 - 1][t2]\n",
        "                c = distances[t1 - 1][t2 - 1]\n",
        "\n",
        "                if a <= b and a <= c:\n",
        "                    distances[t1][t2] = a + 1\n",
        "                elif b <= a and b <= c:\n",
        "                    distances[t1][t2] = b + 1\n",
        "                else:\n",
        "                    distances[t1][t2] = c + 1\n",
        "\n",
        "    return int(distances[len(token1)][len(token2)])\n",
        "\n",
        "input_path = 'drive/MyDrive/nlp_projekt/news.2021.en.shuffled.deduped'\n",
        "output_path = 'drive/MyDrive/nlp_projekt/levensthein' + str(sentence_limit) + '_confusion_set.json'\n",
        "input_example = 'example.txt'\n",
        "output_example = 'example_out.json'\n",
        "\n",
        "sentence_limit = 10000\n",
        "\n",
        "spacy_en = spacy.load('en_core_web_lg')\n",
        "english_word_regex = r\"([A-Z]|[a-z]|')\"\n",
        "\n",
        "\n",
        "def max_distance_criterium(word1, word2):\n",
        "    max_distance = 3\n",
        "    if len(word1) <= 2 or len(word2) <= 2:\n",
        "        max_distance = 2\n",
        "    return max_distance\n",
        "\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "\n",
        "def create_set(input_file, output_file, sentence_limit):\n",
        "    # dict containing word as a key, and confusing words as a value\n",
        "    new_confusion_dict = {}\n",
        "    unique_words = set()\n",
        "    with open(input_file, encoding=\"utf-8\") as f:\n",
        "        print(\"Reading \" + str(sentence_limit) + \" sentences from a file...\")\n",
        "        for i in range(sentence_limit):\n",
        "            # reading all the file at once may end in memory error\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            line = line[:-2]    # remove \\n\n",
        "            list_of_words = [tok.text for tok in spacy_en.tokenizer(line)]\n",
        "            for word in list_of_words:\n",
        "                if re.match(english_word_regex, word):\n",
        "                    unique_words.add(word)\n",
        "\n",
        "    unique_words_list = list(unique_words)\n",
        "    print(\"Create dictionary. Words to process: \" + str(len(unique_words_list)))\n",
        "    start_in = datetime.now()\n",
        "    for i in range(len(unique_words_list) - 1):\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "            print(\"already processed \" + str(i) + \" words in \" + str(datetime.now() - start_in) + ' time...')\n",
        "            start_in = datetime.now()\n",
        "        confusion_list = []\n",
        "        if len(unique_words_list[i]) < 2:\n",
        "            continue\n",
        "        for j in range(len(unique_words_list) - 1):\n",
        "            if len(unique_words_list[j]) < 2 or abs(len(unique_words_list[i]) - len(unique_words_list[j])) > 2:\n",
        "                continue\n",
        "            if i != j and levenshtein_distance(unique_words_list[i], unique_words_list[j]) < \\\n",
        "                    max_distance_criterium(unique_words_list[i], unique_words_list[j]):\n",
        "                confusion_list.append(unique_words_list[j])\n",
        "        new_confusion_dict[unique_words_list[i]] = confusion_list\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as output:\n",
        "        output.write(json.dumps(new_confusion_dict))\n",
        "\n",
        "\n",
        "create_set(input_path, output_path, sentence_limit)\n",
        "# create_set(input_example, output_example, sentence_limit)\n",
        "print(\"Whole process took: \" + str(datetime.now() - start))\n"
      ],
      "metadata": {
        "id": "YouqllhmqSUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241ea92d-a410-44a2-e234-3124e09a245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 10000 sentences from a file...\n",
            "Create dictionary. Words to process: 29256\n"
          ]
        }
      ]
    }
  ]
}