{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_words_correction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwuO8DlVm3mUUc7Z4/H1n0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radamp11/nlp_words_correction/blob/master/nlp_words_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "rUy8d7tiipdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P7yPOHNga1P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello world')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc0SE4yrgkuc",
        "outputId": "9bc3d383-02a2-455f-a75e-df98fbd8ce5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # if we need e.g. saving to spreadsheets on google drive\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "# # if we need to connect our google drive for e.g. load text corpora\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LaJ5r5F6pWBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOOLS INSTALLATION"
      ],
      "metadata": {
        "id": "ZxjeHk1PizoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing essentiall tools via !pip install -U"
      ],
      "metadata": {
        "id": "k6Sc80whi17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCABULARY AND DATASET CLASS\n"
      ],
      "metadata": {
        "id": "Ao8FZBEQiwjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reads words from corpora and embeds them into tokens using sentencepiece and collects them in vocabulary dict\n",
        "class Vocabulary:\n",
        "    pass"
      ],
      "metadata": {
        "id": "OFT369OUkQCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uses Vocabulary class; returns item via __getitem__\n",
        "class WordsDataset(Dataset):\n",
        "    def __init__(self, root_dir, text_corpora_file, freq_threshold=5):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        pass"
      ],
      "metadata": {
        "id": "6kYuFnvXi72z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional - may be not necessary\n",
        "# collate is needed for batch processing to fill the len difference between sentences with <PAD> token to make them the same length \n",
        "class Collate:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Sbxf7_TKmOBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL"
      ],
      "metadata": {
        "id": "_gUfbxqDi4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, input): \n",
        "        # TODO\n",
        "        input = self.dropout(input) # embed?\n",
        "        hiddens,  = self.lstm(input)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Z2oi2JANjVYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARAMETERS"
      ],
      "metadata": {
        "id": "P1WEfuIxndRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 1024 #?\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "dropout = 0.4\n",
        "vocab_size = 0 # len of dataset\n",
        "\n",
        "checkpoint_filename = (f\"date:{str(datetime.datetime.now())}\"\n",
        "                       f\"_embed:{str(embed_size)}\"\n",
        "                       f\"_hidden:{str(hidden_size)}\"\n",
        "                       f\"_lay:{str(num_layers)}\"\n",
        "                       f\"_lr:{str(learning_rate)}\"\n",
        "                       f\"_epochs:{str(num_epochs)}\"\n",
        "                       f\"_batch:{str(batch_size)}\"\n",
        "                       f\"_drop:{str(dropout)}\"\n",
        "                       f\".pth.tar\")"
      ],
      "metadata": {
        "id": "x5jRXNjjnczX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADERS"
      ],
      "metadata": {
        "id": "M_lzj8z8m12p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(root_dir, text_corpora_file, freq_threshold):\n",
        "    loader = DataLoader()\n",
        "    dataset = WordsDataset(root_dir, text_corpora_file, freq_threshold)\n",
        "    return loader, dataset"
      ],
      "metadata": {
        "id": "5rMpv1EHnAph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, dataset = get_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "unjIbYb0m3UV",
        "outputId": "d5d5ac97-23f7-4d11-a586-ebb2ebde8c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7249072817ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_loader() missing 3 required positional arguments: 'root_dir', 'text_corpora_file', and 'freq_threshold'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "x_XAkTt4oBZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "start = datetime.datetime.now()\n",
        "print(\"Training initialized at: \", start)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for idx, (stuff_from_getitem_method1, stuff2) in enumerate(train_loader):\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            stuff_from_getitem_method1 = stuff_from_getitem_method1.to(device)\n",
        "            stuff2 = stuff2.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, _ = model(stuff_from_getitem_method1, stuff2)\n",
        "\n",
        "            # loss = criterion(outputs.reshape(-1, outputs.shape[2]), reference_text_with_no_errors[1:].reshape(-1))\n",
        "            # loss.backward(loss)\n",
        "            optimizer.step()   \n",
        "\n",
        "    # saving to checkpoint step\n",
        "    if epoch % 10 == 0 and epoch != 0:\n",
        "        torch.save([model, optimizer], 'path_to_your_google_drive' + f'epoch{str(epoch)}_' + checkpoint_filename)\n",
        "\n",
        "    # validate(model, device, val_loader, dataset, f'add_att_1024_1layer_fixed_loss/add_att_1024_1layer_epoch{str(epoch)}_loss{str(round(loss.item(),6))}_' + result_worksheet_name, vis_att=True)\n",
        "    model.train()\n",
        "        \n",
        "    # print(\"Epoch no. \", epoch, \", timestamp: \", datetime.datetime.now())\n",
        "    # print(\"Loss: {:.6f}...\".format(loss.item()))\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(f\"The training took {end - start} seconds\")"
      ],
      "metadata": {
        "id": "rnKmSiEmoDDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}